{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python >= 3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3,5)\n",
    "\n",
    "# Scikit-Learn >= 0.20 is required\n",
    "import sklearn \n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# common imports\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATASET_PATH = os.path.join(PROJECT_ROOT_DIR, \"wifi_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. <font color=green>Data Loading</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is split into 3 parts (train, validation and test) and stored in a directory located at DATASET_PATH\n",
    "The data is stored in the csv format. Each split contains some number of csv files. Each csv file in the TRAIN and VALIDATION splits contains 223 columns: the first 220 represent the access points from which the sensor data was received; the last 3 are the ground truth values on the x,y,z location. Note, there is NO header with column names in any of the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to csv files\n",
    "path_to_train_csvs = os.path.join(DATASET_PATH, \"train\")\n",
    "path_to_val_csvs = os.path.join(DATASET_PATH, \"val\")\n",
    "path_to_test_csvs = os.path.join(DATASET_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one of the train csv files, note it does not contain a header with column names\n",
    "some_csv = pd.read_csv(os.path.join(path_to_train_csvs, \"1.csv\"), names = list(range(0, 220))+['x', 'y', 'z'])\n",
    "some_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each csv file in the TEST split contains only 220 columns that represent the access points from which the sensor data was received; there are NO columns containing the ground truth values on the x,y,z location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one of the test csv files, note it does not contain a header with column names\n",
    "some_csv = pd.read_csv(os.path.join(path_to_test_csvs, \"1.csv\"), header=None)\n",
    "some_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Write a function called <font color=blue>build_feats</font>, that constructs only a feature matrix.\n",
    "The function takes only one parameter, a string containing the path to the data (csv files)\n",
    "and returns a feature matrix of type np.ndarray\n",
    "\n",
    "The function will be used to build test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feats(path_to_csvs):\n",
    "    #TODO: implement the function\n",
    "    \n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_test = build_feats(path_to_test_csvs)\n",
    "\n",
    "# verify that the returned value is of type numpy.ndarray\n",
    "assert(isinstance(feats_test, np.ndarray))\n",
    "\n",
    "# verify dimensions of the returned feature matrix\n",
    "assert(feats_test.shape == (2601,220))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Write a function called <font color=blue>build_feats_targets</font>, that constructs a feature matrix and the corresponding target vector. \n",
    "The function takes only one parameter, a string containing the path to the data (csv files),\n",
    "and returns a tuple containing the feature matrix and the target vector, both of type numpy.ndarray. Remember, the csv files contain both features and target values. The function should read the data from each csv file and concatenate the corresponding portions.\n",
    "\n",
    "The function will be used to build train and validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feats_targets(path_to_csvs):\n",
    "    #TODO: implement the function\n",
    "    \n",
    "    \n",
    "    return feats, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = build_feats_targets(path_to_train_csvs)\n",
    "\n",
    "# verify that the returned value is indeed a tuple\n",
    "assert(isinstance(train_data, tuple))\n",
    "\n",
    "feats_train, targets_train = train_data\n",
    "\n",
    "# verify that the returned tuple elements are indeed numpy.ndarray\n",
    "assert(isinstance(feats_train, np.ndarray))\n",
    "assert(isinstance(targets_train, np.ndarray))\n",
    "\n",
    "# verify dimensions of the returned feature matrix and a target vector\n",
    "assert(feats_train.shape == (6049,220))\n",
    "assert(targets_train.shape == (6049,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = build_feats_targets(path_to_val_csvs)\n",
    "\n",
    "# test that the returned value is indeed a tuple\n",
    "assert(isinstance(val_data, tuple))\n",
    "\n",
    "feats_val, targets_val = val_data\n",
    "\n",
    "# verify that the returned tuple elements are indeed numpy.ndarray\n",
    "assert(isinstance(feats_val, np.ndarray))\n",
    "assert(isinstance(targets_val, np.ndarray))\n",
    "\n",
    "# verify dimensions of the returned feature matrix and a target vector\n",
    "assert(feats_val.shape == (1976,220))\n",
    "assert(targets_val.shape == (1976,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. <font color=green>Predicting of a user's coordinates using randrom forest regression</font>\n",
    "\n",
    "\n",
    "Random forest is a supervised learning algorithm which can be utilized for both classification and regression problems. It combines multiple decision trees that train on various subsets of given data. The trees are run independently from each other, and in parallel. The output of the random forest is the mode of the class predictions or the mean of the value predictions, for classification and regression tasks respectively.\n",
    "\n",
    "Today we will use a random forest regressor for indoor localization by predicting the x,y,z coordinate values of a user based on the 220 signal values available at that point.\n",
    "\n",
    "\n",
    "See the documentation on the list of parameters to tune https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# can be updated to include a greater variety of parameters and their values to explore\n",
    "param_grid = [\n",
    "    {'bootstrap': [False], 'n_estimators': [10, 20], 'max_features': [5, 10]},\n",
    "  ]\n",
    "\n",
    "# please fix the value of random_state to 42 for reproducibility\n",
    "forest_reg = RandomForestRegressor(n_jobs=10, random_state=42)\n",
    "\n",
    "# we recommend using the exhaustive search over specified parameter values,\n",
    "# but feel free to explore other approaches\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(feats_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can check out the result details in a dataframe format\n",
    "cvres = grid_search.cv_results_\n",
    "pd.set_option(\"max_colwidth\", 80)\n",
    "df_cvres = pd.DataFrame(cvres)\n",
    "df[[\"params\", \"mean_test_score\", \"std_test_score\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>NOTE !<font> \n",
    "Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root.\n",
    "\n",
    "    \n",
    "From Geron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras and TensorFlow: concepts, tools, and techniques to build intelligent systems (2nd ed.). O’Reilly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(\"MSE={} for model parameters {}\".format(np.sqrt(-mean_score), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or simply retrieve the parameters that gave the best score\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Write a function called <font color=blue>euclidean_distance</font>\n",
    "The function takes two parameters, a vector of targets (or ground truth values) and a vector of predictions,\n",
    "and returns the euclidian distance between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(targets, preds):\n",
    "    # TODO: implement the function\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Choose your best estimator and email it to us to win the competition.\n",
    "\n",
    "The target values for the test set are withheld for the competition purposes. Instead, you are given the validation set to verify your best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "forest_preds = grid_search.best_estimator_.predict(feats_val)\n",
    "\n",
    "forest_mse = mean_squared_error(targets_val, forest_preds)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_euc = euclidean_distance(targets_val, forest_preds)\n",
    "\n",
    "print(\"Random Forest. RMSE: {:.3f}, MSE: {:.3f}, ED: {:.3f}\".format(forest_rmse, forest_mse, forest_euc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email the predictions of your best estimator on the test features by <font color=red>11:30 AM UTC+6 June 25, 2021</font> to <font color=blue>issai@nu.edu.kz</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"John\" # change to your first name\n",
    "surname = \"Snow\" # change to your lastname\n",
    "forest_preds = grid_search.best_estimator_.predict(feats_test)\n",
    "\n",
    "# email your csv file to issai@nu.edu.kz\n",
    "pd.DataFrame(forest_preds).to_csv(\"{}_{}.csv\".format(name, surname), header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
